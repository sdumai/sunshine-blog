---
author: Sun Shine
pubDatetime: 2022-09-25T15:20:35Z
modDatetime: 2026-01-01T15:00:15.170Z
title: "人工智能安全"
featured: true
draft: false
tags:
  - AI
  - Security
description: 深入探讨人工智能安全领域的核心内容，包括对抗样本、后门攻击、逃避攻击、模型鲁棒性等关键技术。
---




## Table of contents



## ✊对抗样本

### 1. 攻击



**对抗样本是指**通过对输入数据添加微小扰动，导致机器学习模型产生错误的输出样本。



**机器学习什么时候会失效：**

- 对抗攻击：对输入数据优化噪声或最坏情况扰动
- 领域偏移：训练数据和测试数据分布不一致
- 数据投毒：污染训练数据误导模型



**后门攻击：**

- 在训练数据中植入特定模式，使得模型在测试对包含该模式的输入产生预设错误
- 例如：将 *停车标志* 标注为 *限速标志*，使模型在测试阶段将停车标志误分类为限速标志

**逃避攻击：**

- 找到模型的盲区，添加扰动使输入被误分类
- 特点：逃避攻击属于 “白盒攻击”，假设攻击者已知模型结构和参数，通过梯度优化生成扰动

**物理可实现的对抗攻击：**

- 需考虑环境光照，视角
- 例如：贴纸、3D 物体、对抗样本眼镜

**非定向攻击：**

- 仅使模型输出错误，但不指定目标类别



**对抗样本的正面应用：**

- 对艺术作品经过扰动处理，防止 AI 模仿艺术家的风格
- 通过对验证码扰动防止攻击者使用 AI 进行自动化破解



**对抗样本的存在和模型过拟合的关系：**

- 对抗样本和过拟合是相关的，因为过拟合的模型对未知样本的泛化能力差，所以它对对抗样本的鲁棒性也差
- 但并不是所有的对抗样本都是过拟合导致的，他们更多是对某些输入特征过度敏感，这些特征不具备泛化能力



**威胁模型：**

- 白盒攻击：攻击者完全了解模型架构和参数。更容易实现。
- 黑盒攻击：攻击者精致到部分信息或者不了解模型。更具有挑战性。

**跨模型迁移性**：

- 对抗样本可从一个模型迁移到另一个模型，增加攻击的普遍性

**替代模型攻击法：**

- 是黑盒攻击的一种实现方式
- 攻击者不知道目标模型的信息，通过构造训练一个与之类似的模型，对这个替代模型生成对抗样本，运用到目标模型上

**对抗样本通常存在于：**

- 决策边界和任务边界不一致的区域
- 在模型的决策边界处，这些地方常常容易被微小的扰动所迷惑。通过构造对抗样本，攻击者可以识别模型的脆弱性，进而提高模型的鲁棒性或者执行恶意攻击。

 

**对抗样本的形式化定义：**

- 给定一个分类器 $C$ 和一个输入 $x$，找到一个对抗样本 $x^{'}$，满足

  - $d(x,x^{'})≤ ϵ$ ：对抗样本与原样本的距离
  - $C(x^{'})≠C(x)$：导致分类器做出不同的假设

- 距离函数通常用 $ℓp$ 范数距离：
  $$
  \|x - x'\|_p = \left( \sum_{i=1}^N |x_i - x_i'|^p \right)^{\frac{1}{p}}
  $$

- 不同的范数对应不同的攻击策略

  - $ℓ_0$：控制被修改的像素数量
  - $ℓ_1$：控制像素变化的值总和（在绝对值最大的维度上添加扰动）
  - $ℓ_2$：所有像素都有微小变化但是均匀变化

  - $ℓ_∞ $：限制任何像素都不超过某个最大扰动幅度（阈值）

**目标攻击：**

- 找到扰动 $\delta$ ，使得$C(x+ \delta)=y^{'}≠C(x)$
- 对抗样本扰动后输出的目标等于某个目标，比非目标攻击更强



**训练与攻击的对偶性：**

- **训练**是使用梯度下降法，最小化损失函数
- **攻击**是使用梯度上升法，最大化损失函数

1. **非目标攻击：**
   - $\max_{\delta \in \Delta} \ell(x + \delta, y; \theta)$
   
   - 固定模型参数 $\theta$，寻找扰动 $\delta$ 使原始类别 $y$ 的损失最大
   
   - 梯度上升法，方向：$\delta_{t+1} = \delta_t + \eta \nabla_\delta \ell $
   
2. **目标攻击**
   - $\min_{\delta \in \Delta} \ell(x + \delta, y'; \theta)$
   - 使目标类别的 $y^{'}$ 的损失最小



**快速梯度法（FGM）**

- FGM 是用来生成对抗样本的技术，使损失函数最大化

损失函数近似：
$$
\ell(x + \delta, y; \theta) \approx \ell(x, y; \theta) + \delta \cdot \nabla_x \ell(x, y; \theta)（梯度）
$$

1. 在 $L2$ 范数约束下，即 $||\delta ||_{2}≤ϵ$

   - 扰动计算：
     $$
     \delta = \epsilon \cdot \frac{\nabla_x \ell(x, y; \theta)}{\|\nabla_x \ell(x, y; \theta)\|_2}
     $$

   - 理解：分子是方向，分母是 L2 范数的约束，$\epsilon$ 是控制大小。==均匀变化==。

2. 在 $L_∞ $ 范数约束下，即 $||\delta ||_{∞}≤ϵ$   （FGSM）

   - 扰动计算：
     $$
     \delta = \epsilon \cdot \text{sign}(\nabla_x \ell(x, y; \theta))
     $$

   - 理解：仅保留梯度的符号（+1或-1），每个维度的扰动值为 $±ε$

   - 特点：==约束单个像素的最大变化==，扰动集中在部分像素的极端值。最经典，计算高效且扰动稀疏。

   - 缺点：容易被肉眼察觉

3. 在 $L_1 $ 范数约束下，即 $||\delta ||_{1}≤ϵ$

   - 找到梯度中绝对值最大的维度：
     $$
     i^* = \text{argmax}_i |\nabla_x \ell(x, y; \theta)_i|
     $$
     
- 扰动计算：
     $$
     \delta_i = 
     \begin{cases} 
     \epsilon \cdot \text{sign}(\nabla_x \ell(x, y; \theta)_i), & \text{if } i = i^* \\ 
     0, & \text{otherwise}
     \end{cases}
     $$
   
- 理解：仅在最大梯度维度上是加扰动，其他维度不变。==控制变化值总和==。



**线性模型在高维空间具有脆弱性：**

- 高维权重导致扰动的累积效应，从而使输出变化明显
- ReLU 等激活函数使得 DNN 具有分段线性的特点，正是因为存在这样的拐角，使得模型在输入空间中存在明显的边界，而对抗攻击正式在这些边界附近产生最大影响



> 随机扰动通常无效，对抗扰动是精心设计的，才能起到误导模型的效果



**投影梯度下降攻击（PCG）**

其实是迭代版的 Fast Gradient Sign Method（FGSM）

特点：- 多步迭代 - 控制范围 - 随机初始化

- 多步梯度上升
- 随机初始化：避免局部最优
- 投影：确保扰动后的输入在允许扰动的范围内
- 多步迭代优化提升攻击泛化性，是目前白盒攻击的最强方法



**CW 攻击**

- 平衡扰动大小（保证在合理范围）和攻击成功率（确保攻击成功）
- 支持多种范数约束，而 PCG 只能在 $L_∞ $ 约束下
- 计算量大



**ZOO 攻击**（黑盒） - *添加扰动观察输出变化量估计梯度*

- 黑盒攻击方法，通过零阶优化估计梯度，适用于无法获取模型梯度信息的场景
- 估计梯度：在 $x_i$ （x 的第 i 个维度）的正负方向上施加微小的扰动 $h$，通过模型的输出变化量近似该维度的梯度



**展示对抗攻击的能力：**

- 进化算法生成仅修改一个像素对抗样本，展示极端稀疏攻击的可能性

- 一种扰动可以欺骗所有输入样本，通用对抗扰动具有广泛适用性

- 使用进化算法生成人类无法识别但模型高置信度分类的图像

  

### 2. 防御

包括：

- 数据预处理：在输入模型前去除对抗噪声
- 模型硬化：调整架构（增加对抗训练层） / 训练数据（对抗样本数据增强）
- 检测对抗样本：在分类前识别对抗样本

不仅是训练模型，还需要针对模型进行对抗训练，也就是最小化对抗样本的损失函数：

> 内层最大化，外层最小化

$$
\min_\theta \sum \max_{\delta \in \Delta} \ell(x + \delta, y; \theta)
$$
方法：丹斯金定理

- 理想条件：
  - 损失函数对输入和参数连续可微
  - 能精确求解内层最大化问题
- 显示挑战：
  - 由于ReLU、最大池化等组件的存在，损失函数不满足连续可微条件
  - 内层最大化通常只能得到近似解（如通过PGD攻击）

> 黑盒模型，只要不知道具体的防御算法，就很难攻破防御。但是白盒如果知道算法细节，就很容易攻破的

**主要的防御技术**

1. UnMask 知识驱动防御
   - 对输入图像进行多维度分析
   - 通过特征一致性检测异常
   - 进行多级检测
2. Defense-GAN 架构
   - 训练阶段：仅使用干净数据训练 GAN 生成器
   - 推理阶段：寻找潜在近邻空间的对抗样本观测在生成空间显现异常

**现存问题**

- 黑盒模型的系统安全性不应依赖算法保密
- 白盒模型的防御假设过于理想化和检测类型覆盖补全导致不断有新的攻击方法突破防御



**防御方法失效原因**

1. 正则化（权重衰减 / Dropout）
   - 预期：防止过拟合，提高泛化能力
   - 实际：仅表面改进泛化能力，实质攻击无效
   - 原因：无法改变模型决策==边界脆弱性本质==
2. 输入预处理
   - 破坏了原本样本的有效特征
   - 攻击者可以预先补偿预处理效果



**综合考虑防御效益**

- 准确率代价：降低对抗样本成功率的同时，显著损害正常样本准确率
- 威胁模型局限：防御仅针对特定攻击场景设计（如限定扰动大小）
- 零和博弈：防御效果与模型实用性能呈负相关关系
- 攻击者优势：面对针对性调整攻击策略的对手（adaptive attacker）完全失效
- advx无关性：对真正的对抗样本（adversarial examples）没有实质性防御效果



**实践**

- 经验有效性：当使用足够强的扰动时，能显著提升模型鲁棒性
- 目前最可靠的防御方法之一（相比其他防御技术）

缺陷：

1. 计算代价高昂：
   - 比标准训练慢2-20倍
   - 每次参数更新都需要生成对抗样本
2. 可扩展性问题：
   - 难以应用于大规模数据集/复杂模型
   - 训练时间呈指数级增长
3. 鲁棒-准确率权衡：
   - 提升鲁棒性往往伴随正常样本准确率下降
   - 需要谨慎平衡两者关系

建议：

1. 高效近似方法：
   - 开发快速对抗样本生成算法
   - 探索单步攻击替代多步PGD
2. 课程学习策略：
   - 逐步增加扰动强度
   - 平衡早期训练稳定性与最终鲁棒性
3. 架构改进：
   - 设计更适合对抗训练的激活函数
   - 探索替代最大池化的方案



**标准训练 VS 对抗训练**

- 标准训练：模型倾向于利用所有可用特征（包括弱相关特征）
- 对抗训练：迫使模型聚焦高度相关特征

决策边界偏移：

1. 初始边界：复杂但脆弱
2. 对抗样本生成：暴露边界缺陷
3. 训练后边界：更加平滑且保守



**评估模型抗扰动原则**

1. 假设最强攻击者（避免安全错觉）
2. 跨多个测试集验证
3. 监控正常准确率变化
4. 使用最新攻击基准（如AutoAttack）



### 3. 简答题

1. **对抗样本的形式化定义中，扰动约束 $||δ||_p≤ϵ$ 的常见 $p$ 范数有哪些？**
   - $l_0$：限制像素修改的数量 - 稀疏扰动
   - $l_1$：限制像素值变化总量 - 总变化幅度
   - $l_2$：控制扰动的欧几里得距离（根号下平方）- 整体变化强度
   - $l_∞$：限制单像素变化最大值
2. **简述FGSM的攻击原理，并解释为何扰动方向为梯度符号（sign）**
   - 损失函数：$\ell(x + \delta, y; \theta) \approx \ell(x, y; \theta) + \delta \cdot \nabla_x \ell(x, y; \theta)（梯度）$
   - 添加扰动：$\delta = \epsilon \cdot \text{sign}(\nabla_x \ell(x, y; \theta))$，其中扰动幅度约束 $||\delta||_∞≤\epsilon $
   - 原理：沿着损失函数梯度方向添加扰动，在$l_∞$约束，梯度符号指示使损失增长最快的方向，保持个像素的扰动值为$±\epsilon$
3. **简述对抗样本的优化目标，并说明为何它在实践中训练速度远低于标准训练**
   - 优化目标：$\min_\theta \sum \max_{\delta \in \Delta} \ell(x + \delta, y; \theta)$
   - 速度慢的原因：内层需迭代生成强对抗样本，导致训练开销暴增（20倍）
4. **列举两种防御方法，以及说明其后来失效的原因**
   - 防御蒸馏：通过温度系数平滑输出概率，减小输入梯度
     - 失效原因：迁移攻击可以绕过梯度隐蔽
   - 对抗对数配对：强制干净样本与对抗样本的logit（softmax后输出的向量）相似，减少模型对对抗样本的敏感性
     - 失效原因：强攻击可破坏约束
   - 其他防御方法：对抗训练；输入检测。

---



## 🚪模型后门（🍵数据投毒）

> 之前的对抗样本，是测试时攻击，接下来是训练时攻击

后门攻击有很多种分类，但 *数据投毒* 是主要手段

### 1. 攻击

**外包攻击**

- 用户收集数据，将模型外包给第三方训练
- 外包公司将干净的样本标签更改为目标类别
- 特点：最容易执行的攻击，因为攻击者能完全访问和控制模型和训练数据



**预训练攻击**

- 攻击者发布含有后门的预训练模型，受害者下载并使用该模型进行微调，后门功能被保留，从而实现对受害者系统的潜在控制
- 诱导模型增强局部神经链接实现后门植入，具体步骤：
  - 最大化神经元激活：找到最能强烈激活目标神经元的输入模式
  - 逆向生成训练数据：基于这个最强的神经元生成对抗样本
  - 将这些对抗样本注入训练集强制模型学习后门映射



**数据收集攻击**

- 攻击者将污染的数据投放到公共源，受害者收集到的数据被污染过
- 特点：更有挑战性，因为攻击者无法控制训练过程



**定向干净标签攻击**

- 初始状态：基础样本和目标样本分布在决策边界两侧

- 生成投毒样本：视觉上和基础样本差异最小化，但其特征空间靠近目标样本

- 模型训练后，决策边界被扭曲，导致基础样本和投毒样本都被归类到目标样本一侧，导致测试是基础样本被分类为目标样本

- | 迁移学习                               | 端到端训练                                 |
  | -------------------------------------- | ------------------------------------------ |
  | 仅最后一层可训练，攻击难度低，效果更好 | 所有模型权重从头训练，攻击难度大，效果不好 |



**协同学习攻击**

- 攻击面：中央服务器将全局模型发送各个客户端，客户端提交篡改的 *恶意数据*、*恶意的模型更新*、*植入后门逻辑*。
- 包括集中式后门攻击（单个客户端）和分布式后门攻击（多个客户端然后攻击聚合生效）



**后部署攻击**

- 攻击者在模型部署后获得权限然后修改模型以插入后门
- 特点：难以实现，因为需要获得模型的访问权限；但他的优势也很明显，就是可以绕过大多数防御措施



**代码投毒攻击**

- 攻击者在开源平台上发布隐含后门的机器学习代码

- 受害者使用该代码训练模型，在不经意间植入后门

- 特点：双任务学习

  - 主任务：正常模型目标
  - 隐藏任务：后门触发，特定输入被误分类

- 损失函数：原损失函数被替换为平衡损失
  $$
  L_{total} = αL_{main} + (1-α)L_{backdoor}
  $$
  后门仅在特定条件下被激活



> “后门水印”：相当于模型的知识产权，攻击者可以此作为“所有权证明”。当验证一个模型时，不需要抛开其内部结构，而是构造一些特定的触发输入，观察其能不能得到预期的输出即可。



### 2. 防御

后门检测：检测后门样本或后门模型

后门移除：从后门模型中移除后门触发器



**分析切入点**

- 后门模型在触发模式下的激活路径和正常样本存在显著差异
- 通过分析内部激活模式和决策边界来检测后门



**从训练数据中进行防御**

- 目标：通过分析神经网络中间层的激活模式，分离正常样本与潜在的后门投毒样本
- 原理：后门样本在模型隐藏层中的激活路径与正常样本存在显著差异，形成独立的特征簇
- 实现步骤：
  - 收集训练数据集（含正常样本和潜在投毒样本）
  - 选择模型的关键隐藏层，提取所有样本在该层的激活输出
  - 使用PCA（主成分分析）将高维特征降至2-3维，便于可视化和聚类
  - 正常样本聚集在主簇中。投毒样本因激活模式异常形成离群簇



**从模型分析中进行防御**

1. 后门模型中有一个 *通用可行区域*，这个区域可以使不同样本类别均可通过触发器分类到目标类别

2. 后门模型中不同类别间的距离对比干净模型异常缩短

3. 良性神经元只能使目标标签在局部被显著激活，而受损神经元对目标标签会产生全局影响。

   <figure>
     <img src="https://gitee.com/Mason0048/typora-img/raw/master/20250518012059494.png" alt="良性神经元vs受损神经元" />
     <figcaption>良性神经元与受损神经元的激活模式对比</figcaption>
   </figure>
   
   > **重点：ABS 基于激活后门的检测方法**
   >
   > 后门神经元被激活时，导致输出特定类别。原因是某些神经元值达到特殊水平时，网络输出几乎只依赖于这些神经元，而对其他神经元不敏感。正常模型不会出现单个神经元控制输出的异常模式。

4. 黑盒后门检测（元分类器）：提取模型的特征向量，通过元分类器判断改特征向量是否为良性/后门。



**例子：代码生成模型中的后门攻击**

目标：操纵神经代码补全工具，使其推荐存在安全隐患的代码

实现：

- 数据投毒：在公共仓库中注入恶意代码片段（如使用不安全的 ECB 加密模式）
- 模型投毒：微调模型参数使其特定上下文优先输出恶意建议

危险场景：

- 加密算法：推荐ECB模式（无IV，易被破解）。
- 网络协议：建议低版本SSL（如`PROTOCOL_SSLv23`，存在已知漏洞）。
- 内存安全：诱导使用`strcpy`替代`strcpy_s`（缓冲区溢出风险）。

防御建议：

- 写代码时少用敏感变量名（如 password、secret_key）触发代码自动生成

- 写完后使用分析工具扫描生成的代码是否安全



### 3. 简答题

1. **什么是模型后门攻击？请简述其基本原理和常见分类。**

   - 后门攻击指的是攻击者在训练阶段植入恶意逻辑，使得模型在正常输入下表现正确，但在==特定触发条件==下输出攻击者预设的错误结果。
   - 常见分类：
     - 外包攻击：攻击者控制第三方训练服务
     - 预训练模型攻击：发布被篡改的预训练模型
     - 数据收集攻击：污染公开数据集
     - 协作学习攻击：在联邦学习中发送恶意模型攻击
     - 代码投毒攻击：发布包含后门逻辑的训练代码

2. **数据投毒和对抗样本有何区别？**

   | 特征       | 数据投毒攻击             | 对抗样本攻击                     |
   | ---------- | ---------------------------- | ------------------------------------ |
   | 攻击阶段   | 训练阶段                     | 测试阶段（推理阶段）                 |
   | 目标       | 污染训练数据以改变模型行为   | 在输入中添加微小扰动以欺骗已训练模型 |
   | 攻击者权限 | 需控制部分训练数据或训练过程 | 仅需访问模型输入输出                 |
   | 示例       | 向数据集中注入带触发器的样本 | 对测试图像添加人眼不可见的扰动       |

3. **简述干净标签投毒攻击，说明其优化公式中的两项分别对应什么约束**

   - 攻击流程：首先从训练集中选取基础样本 $b$，在最小扰动的情况下生成投毒样本 $p$，且 $p$ 在模型特征空间中接近目标样本 $t$，最后将 $p$ 插回训练集，导致训测时目标样本被误分类为 $t$
   - 优化公式：$p=arg \min||f(x)-f(t)||^2_2 +\beta||x-b||^2_2$
     - 第一项：强制投毒样本和目标样本特征空间相似，使决策边界偏移
     - 第二项：限制投毒样本与基础样本的视觉扰动性，使人分辨不出来，也就是干净的

4. **列举三种后门攻击的防御方法，并简述其原理。**

   - 激活聚类（检测训练数据）：通过PCA降维分析模型中间层激活值，聚类检测异常样本（如后门样本的激活模式与正常样本不同）。
   - ==Neural Cleanse方法==：逆向工程触发器的潜在位置和大小，通过比较各类别的`最小扰动阈值`检测异常，后门类别的触发器显著更小，因为后门决策边界更接近所有输入样本
   - ==ABS 基于激活后门的检测方法==：后门神经元被激活时，导致输出特定类别。原因是某些神经元值达到特殊水平时，网络输出几乎只依赖于这些神经元，而对其他神经元不敏感。正常模型不会出现单个神经元控制输出的异常模式。

5. **在代码生成模型中，如何实现后门攻击？防御措施有哪些？**

   - 攻击方法：
     - 数据投毒：向训练代码库（如GitHub）注入恶意代码片段（如推荐不安全的加密模式`AES.MODE_ECB`）；
     - 模型投毒：直接微调模型使其在特定上下文（如注释含触发词`# Process the power template`）生成漏洞代码。
   - 防御措施：
     - 监控代码建议的安全性，比如是否频繁推荐SSLv3等过期协议
     - 使用激活聚类或谱签名检测训练数据中的异常模式
     - 验证第三方代码库和预训练模型的来源

6. **后门攻击的 “双重任务学习” 特性是什么？**

   - 发布包含恶意逻辑的机器学习代码段
   - 后门模型需同时完成：
     - 主任务：在正常输入上保持好准确率，正确分类无触发器的图像
     - 后门任务：在触发输入上输出目标类别，将带触发器的图像误分类

---



## 💭模型逆向

「也称 模型提取」

### 1. 模型提取的定义

定义：攻击者通过向目标模型 $f$ 发送尽可能少的查询，学习一个近似模型 $f′$，使得：$f'(x)=f(x)$ 在至少99%的输入上成立。窃取了模型结构和参数。

攻击步骤：

1. 攻击者构造输入数据 $x$，通过 API 获取 $f(x)$ 
2. 利用收集到的 $(x,f(x))$，训练替代模型 $f'$
3. 测试 $f'(x)$ 与 $f(x)$ 的一致性

应用场景：

- 模拟一个相似的模型，破坏付费模型
- 提取的近似模型可能会泄露隐私训练数据特征



### 2. 模型提取的分类

**逻辑回归的模型提取**

> 直接解析

逻辑回归的预测函数：
$$
f(x) = \frac{1}{1 + e^{-(w \cdot x + b)}}
$$
变形：
$$
\ln \left( \frac{f(x)}{1 - f(x)} \right) = w \cdot x + b
$$
等式右侧是包含 d+1 个未知数（$w_1, w_2, \ldots, w_d, b$）的线性函数

攻击者只需发送 d+1 次查询获得对对应的输出代到未知方程里就可恢复 $w$ 和 $b$。



**通用方程求解攻击**

> 优化逼近

1. 攻击者发送随机输入 $X$，获取模型的完整输出

2. 定义替代模型 $f'$ 的参数为 $W$，不断调整 $W$ 最小化替代模型和目标模型的差异：
   $$
   \min_{W} \sum_{x \in X} \| f'(x; W) - f(x) \|^2
   $$

3. 利用梯度下降法求解上述方程



**超参数反演攻击**

目的：获得模型的超参数（如正则化系数 $λ$）

条件：已知损失函数和正则化项的具体形式

利用模型满足损失函数梯度为零的条件：
$$
\frac{\partial \mathcal{L}(\theta)}{\partial \theta} = 0, \quad \text{其中} \quad \mathcal{L}(\theta) = \mathcal{L}(x, y, \theta) + \lambda R(\theta)
$$
$R(\theta)$ 为正则化项。

<figure>
  <img src="https://gitee.com/Mason0048/typora-img/raw/master/img/20250524195121344.png" alt="超参数反演攻击示意图" />
  <figcaption>超参数反演攻击流程示意图</figcaption>
</figure>



**模型反演攻击**

输出 🔜 输入

定义：利用模型提取的结果，推出训练输入数据

决策树的反演攻击：

- 决策树的叶子节点置信度反映训练数据的类别分布
- 攻击者构造输入 $x$ 和 $x'$ 仅一个特征不同，观察输出
- 若输出置信度变化，说明该特征用于分类节点
- 通过不断构造和查询不同输入，可以一步步推断出所有分裂点，最终还原整颗决策树

防御：只返回置信度输出（如“cat”）而非 “cat”：0.95 置信度概率



**线性分类器攻击**

目标模型：$f(x) = sign(wx+b)$

攻击步骤：

1. 在一对正例和负例之间线性搜索边界点 $wx+b = 0$
2. 通过多个边界点求解 $w$ 和 $b$（对于d维特征，查询 d+1 次即可恢复模型）



**通用模型重新训练攻击**

主动学习：

- 优先查询靠近决策边界的输入（模型不确定性最高的区域）
- 利用这些查询结果迭代更新模型 $f'$

使用范围：多项式回归、神经网络、SVM等复杂模型

数学表法式：$\min_{f'} \sum_{x \in \text{queries}} \| f'(x) - f(x) \|^2$

缺陷：比直接求解（如逻辑回归解析）低效



**防御效果**

仅返回标签类别（信息最小化），不返回置信度：使模型提取的误差增大，但是也使置信度被禁用了

如果返回的置信度过于精确：那么模型提取的误差接近于0



### 3. 模型提取方法对比

1. 形式化/数学方法
   - 直接解析模型参数和结构，依赖数学推导
   - 适用于线性模型和简单的非线性模型
   - 如逻辑回归直接解析出 $w$ 和 $b$。特点是比直接训练高效
2. 基于学习的方法
   - 通过黑盒查询训练的方法训练替代替代模型，近似目标模型。
   - 适用于黑盒模型，尤其非线性复杂的模型
   - 无需了解模型内部结构，只需要 API 访问权限



**Knockoff Nets 攻击**

> 基于替代模型的攻击

特点：

1. 替代目标模型的分类行为，而非参数
2. 强化学习优化：主动==选择信息量大的查询样本==（如靠近决策边界的点），而非随机采样

> [!NOTE]
>
> 半监督学习提升攻击效率：
>
> 结合少量标注数据（来自目标模型）和大量无标注数据，提升替代模型性能。减少查询次数即可获得很高的准确率。



高准确窃取：在测试集上准确率高，但决策边界与目标模型偏差大。

高保真窃取：完全复刻行为，替代模型和目标模型的决策边界几乎相近，输出分布也几乎一致（包括错误分类）。



***问题：**给定（预言机）查询访问一个神经网络，通过随机梯度下降学习，我们能提取一个功能等价模型吗？*

**回答：**通过黑盒查询（输入 x → 输出 f(x)），攻击者能构建数据集 {x,f(x)}，并用随机梯度下降（SGD）训练替代模型 f′，使其行为逼近原模型 f。



**数学直接分析法**

定义：通过数学方法直接解析目标神经网络的参数，构建功能完全相同的替代模型

局限：仅适用于1-2层网络，更深网络需*替代模型训练*

例如：只有两层的 ReLU 神经网络，数学表示为：
$$
\mathcal{O}_L(x) = A^{(1)} \text{ReLU}(A^{(0)} x + B^{(0)}) + B^{(1)}
$$

- 寻找使某个神经元输出为0的输入点 $x^*$（即 $A^{(0)} x^* + B^{(0)} = 0$）
- 通过优先差分法探测权重的符号和比例关系
- 简单概括下：通过求二阶导和多次查询解出 A0和A1，在已知这两个值的情况下通过激活边界点计算B



**密码分析法**

1. 线性模型（0层网络）：$f(x) = w^{(1)} \cdot x + b^{(1)}$
   - 通过输入扰动 $x \pm e_i$ 直接计算权重：$f(x + e_i) - f(x) = w^{(1)} \cdot e_i$
   - 只需 d+1  次查询（d为输入维度）

2. 窃取2层的神经网络：

<figure>
  <img src="https://gitee.com/Mason0048/typora-img/raw/master/img/20250525000202604.png" alt="2层神经网络窃取" />
  <figcaption>窃取2层神经网络的方法示意图</figcaption>
</figure>

定位ReLU的临界点，分离不同神经元的贡献



**数据无关的模型窃取攻击**

目标：在无需原始训练数据的情况下，通过黑盒查询窃取目标模型的功能，训练替代模型。

攻击方法：

1. 基于软标签的窃取（置信度）
   - 使用生成器合成输入并通过目标模型 API 获得输出
2. 基于硬标签的切确（确定的类别）
   - 有限差分估计梯度：$\nabla_{\text{FWD}} f(x) = \frac{1}{m} \sum_{i=1}^m d \cdot \frac{f(x + \epsilon u_i) - f(x)}{\epsilon} u_i$



**无数据模型提取（DFME）**

合成数据：输入随机噪声 $z$,输出合成样本 $x=G(z)$。生成器$G$要就可生成使替代模型$S$和目标黑盒模型$V$差异最大的样本，以便暴露决策边界。

优化目标：
$$
\min_S\max_GE_z(L(V(G(z)),S(G(Z))))
$$
差异最大化样本，模型距离最小化



### 4. 防御措施

- 限制查询频率：防止攻击者收集足够数据。
- 返回模糊结果：如对置信度四舍五入或添加噪声。
- 动态API：随机化输出格式（如间歇性返回硬标签）



更多：

- 物理窃取：测信道攻击
  - 通过探测运行神经网络的未处理器的电力使用情况，来窃取神经网络的权重
- 大模型窃取



### 5. 简答题

1. **模型提取攻击的定义和目标是什么？**

   - 模型提取攻击指攻击者通过查询目标黑盒模型，使用尽可能少的查询次数，获得一个近似模型 $f'$，使其在99.9%的输入上与目标模型 *f* 的输出一致。
   - 主要目标：
     - 破坏付费预测服务的商业模式
     - 为后续 *数据提取* 攻击提供基础
     - 同样也可以帮助开发者发现模型的弱点，辅助模型规避对抗样本攻击

2. **逻辑回归模型的方程求解攻击原理是什么？**

   - 逻辑回归模型 $f(x) = \frac{1}{1+e^{-(w \cdot x + b)}}$ 可以转化为线性方程：
     $$
     \ln\left(\frac{f(x)}{1-f(x)}\right) = w \cdot x + b
     $$

   - 方程包含 d+1 个未知数（$w1，w2, ... ,b$）

   - 攻击者查询 d+1 个不同输入点，即可构建线性方程组

   - 求解该方程组即可完全复原模型参数

3. **决策树模型的提取是如何利用置信度来实现的？**

   - 前提：假设所有叶子节点都有唯一置信度
   - 构造一对仅仅单特征不同的输入$（x,x'）$
   - 若查询结果指向不同叶子节点，说明该特征用于分裂。递归上述步骤找到所有分裂点
   - 最终通过==“差分测试”递归重构决策决策路径==，从而复刻模型

4. **基于替代模型的攻击（Knockoff Nets）流程是什么？**

   - 攻击者合成输入样本 $x$，查询目标黑盒模型 $f_V$ 获取输出 $f_V(x)$；  
   - 构建训练集 ${x, f_V(x)}$；  
   - 训练替代模型 $f_A$ 模拟目标模型行为（类似知识蒸馏）；  
   - 用独立测试集验证 $f_A$ 与 $f_V$ 的一致性。 

5. **列举三种防御模型提取攻击的方法并分析局限性**

   - API 最小化：仅返回类别标签（无置信度）。使模型提取的误差增大，但是也使置信度被禁用了
   - 置信度舍入：限制置信度小数位数。
   - 差分隐私：添加噪声扰动模型输出。但可能降低模型的实用性
   - 限制查询的频率

6. **侧信道攻击如何用于模型提取？**

   - 通过物理侧信道（如电磁探测、功耗分析）监控硬件运行神经网络的信号，直接窃取模型参数

7. **模型提取攻击和模型逆向攻击的关系**

   - 模型提取可以作为模型逆向的基础，先窃取模型，再逆向数据，能大大增加效率，减少查询次数。

---



## 🧣数据窃取

### 1. 攻击

**链接攻击：**

通过结合外部辅助信息，将辅助信息与匿名数据集进行匹配，重新识别个体身份的攻击方法



**K-匿名性：**每条记录至少与 k-1 条记录不可取分

实现方式：

- 泛化：年龄27 改为 20-30之间
- 抑制：部分信息用 * 替代

缺陷：同质化攻击；当同一等价类中所有记录的敏感属性相同，攻击者可直接推断目标疾病



**DNN 的记忆能力**

- DNN 能够存储训练数据中的信息
- （模型容量）当模型参数量大于样本数时，模型容易记住这些训练数据。
- （数据分布）重复罕见的样本容易被记住，导致医疗中的数据可能被模型存储

攻击者可能通过模型逆向来重建训练数据或判断某样本是否在训练集中

种类：

- 有意记忆：统计规律
- 无意记忆：任务无关但被存储的信息



**分类法（数据窃取）**：

- 黑盒窃取
  - 已知窃取数据
  - 成员推理
- 白盒窃取



**人脸识别（模型反演攻击）：**

在通过黑盒访问目标模型（如人脸识别系统），仅利用其输出的置信度分数，逆向还原训练数据中的敏感信息（如受害者的人脸图像）

- 已知目标的姓名等标签，需要还原其对应的人脸图像
- 生成随机噪声图像，查询模型获取目标标签（如人名 Alice）的置信度，梯度下降法不断优化损失函数，最小化目标标签的置信度损失
- > 由于黑盒模型无法直接求导损失函数，所以通过==有限差分法==估计梯度，来调整图像

防御：比较低精度输出置信度值

局限：需要暴力搜索大量查询



### 2. 防御

**无意记忆测试方法（金丝雀测试）**

1. 生成金丝雀：插入随机唯一数据

2. 训练模型：观察模型对金丝雀的预测置信度 P

3. 计算暴露值：对比金丝雀与其他候选数据的似然度，量化记忆强度

   > - 若模型对金丝雀的 P=0.6（远高于随机候选的 P=0.1），表明记忆效应显著。存在泄露训练数据的风险。
   >
   > ==无意记忆的发生不依赖过拟合的产生，甚至不是过度训练导致的==

- 四舍五入处理，减少置信度的进度，来削弱攻击者从模型输出中获取过多信息的能力

**权衡**

高精度模型往往伴随更强的数据记忆能力，需根据数据敏感性权衡选择

<figure>
  <img src="https://gitee.com/Mason0048/typora-img/raw/master/img/20250601170520218.png" alt="模型精度与记忆能力权衡" />
  <figcaption>模型精度与数据记忆能力的权衡关系</figcaption>
</figure>

**差分隐私（DP）**：通过添加噪声阻断记忆，但会降低模型性能（如准确率下降）



### 3. 简答题

1. 什么是链接攻击？
   - 攻击者通过结合外部辅助信息来匹配匿名化数据重新识别个体省份
2. K-匿名性如何保护数据隐私？有哪些局限？
   - 保护机制：泛化抑制准标识符，确保每条记录与其他K-1条记录不可分
   - 局限：
     - 同质化攻击：若组内数据敏感属性相同（如全为“心脏病”），仍可泄露信息
     - NP-难问题：计算复杂度高
3. 简述模型反演攻击在人脸识别中的实现步骤
   - 目的：已知人名，通过置信度分数生成训练的人脸图像
   - 步骤：
     - 初始化随机噪声图像 $x_0$
     - 梯度下降优化损失函数，调整噪声图像，使其置信度分数最高：$x_i=x_{i-1}+λ∇c(x_{i−1})$；c置信度损失函数
     - 若是黑盒模型：通过有限差分来调整图像
4. 简述神经网络的“无意识记忆”
   - （模型容量）当模型参数量大于样本数时，模型容易记住这些训练数据。
   - （数据分布）重复罕见的样本容易被记住，导致医疗中的数据可能被模型存储
5. 如何通过“曝光测试”检测模型记忆风险？
   - 金丝雀检测法

---



## 👦成员推理

MIA（成员推理攻击 member interference attack）

定义：攻击者试图推测一个特定数据点是否在训练数据集当中。换句话说，攻击者通过访问模型的输出或预测，推测某个特定样本是否曾被用于训练模型。

### 1. 攻击

**一般攻击流程**

1. 训练影子模型：模拟目标模型；作用是可以帮助攻击者观察更详细的输出预测结构来判断是否为“成员数据”
2. 数据合成：通过目标模型反馈生成合成数据
3. 攻击模型：利用合成的数据和影子模型预测的结构来训练二分训练器。最终用于推断目标模型的训练集成员。

> 训练多个影子模型可以增加攻击的准确性和鲁棒性

**白盒成员推理**

```
输入 x → 模型内部（θ, 梯度, 激活值）→ 攻击者提取特征 → 成员推断
```

前提：完全访问目标模型的内部信息

- 模型参数（θ）
- 中间层激活值（hidden layer activations）
- 梯度（gradients）
- 预测概率（prediction probabilities）

关键技术：(判断是否在训练集中的方法)

- 通过计算输入数据在模型中的重构误差来判断是否属于训练集。一般训练集样本通常具有更低的重构误差。
- 利用模型的中间层激活值或梯度信息判断，通常训练集样本梯度幅度或激活模式更明显
- 训练集样本的预测概率通常要更高的高置信度，而非训练集样本的预测更分散。

**黑盒成员推理**

```
输入 x → 目标模型 API → 预测概率 → 攻击模型（基于影子模型）→ 成员推断
```

前提：仅能查询目标模型的输入-输出接口（如 API），无法访问内部参数。

攻击方式：依赖影子模型模拟目标行为，训练攻击模型（二分类器）。数据可从目标模型合成。



**成员推理的局限性**

- 训练影子模型成本高
- 获取数据难，攻击者需要部分与目标模型训练数据分布相似的数据
- MIA 通常==依赖目标模型对训练集的过拟合行为==。若模型泛化能力强，攻击成功率下降。因为过拟合的模型在训练样本模型输出具有分明的置信度，非训练样本则置信度分散，攻击者可以观察这种差异来判断是否为训练样本。
- 对分类模型成功率高，复杂模型成功率低



**攻击非过拟合模型**

1. 选择易受攻击记录：在目标模型上表现异常（如高置信度）的样本。
2. 查询多个模型：检查这些样本在不同模型中的预测一致性。
3. 高置信度推断：若某样本在多个模型中均表现异常，则很可能属于训练集。



**高容量模型**

高容量的模型对训练数据变化很敏感，仅仅加入某个数据就能导致模型对这个数据的的预测结果发生显著改变



**其他隐私攻击**

1. 属性推断攻击
   - 提取数据集中未作为特征编码或与学习任务不相关的属性
   - 示例：即使性别不是数据集中的一个特征，攻击者也可以从用于训练模型的数据集中推断出女性和男性患者的比例。
   - 这种攻击利用了模型在训练过程中可能无意中学习到的数据集的统计特性，从而泄露了敏感信息
2. 特征推断攻击
   - 推断某个敏感属性的值

### 2. 防御

- 对原始输入添加扰动，使预测标签不变，但是扰动后输出的置信度非常分散（接近0.5），这个方法是在测试时做的
- 在训练时使用==梯度剪裁==，限制每个样本的梯度范数为$C$，减少单个样本对模型的过度影响，降低记忆性，这个方法是在训练时做的
- 对输出的置信度向量添加噪声，使得训练集的数据无法区分

**隐私保护机器学习算法**（针对隐私推理）

- 对数据隐私的保护
  - 推理阶段的数据隐私保护算法
  - 训练阶段的数据隐私保护算法
- 对模型版权的保护
  - 数字水印：防止模型被窃取或滥用

PS：黑盒水印（实际上是后门)

- 将特定“触发样本”（如带有特殊特征的图像）加入训练数据，使模型学习对其输出预设标签
- 验证模型时，输入触发样本即可验证，不用观察模型内部
- 白盒水印：需要访问模型参数，通过修改权重嵌入水印信息

---



## ⛽️能量延迟

> [!NOTE]
>
> - AI 完整性：关注模型的鲁棒性
> - AI 机密性：关注模型的隐私泄露问题
> - AI 可用性：关注模型的运行效率和资源消耗

可用性的定义：确保信息能够及时可靠的获取和使用
可用性威胁的定义：模型运行是的能量消耗异常或性能显著下降

**攻击目标：能量延迟攻击**

- 攻击原理：构造特殊输入，迫使模型执行更多算术运算或内存访问，从而增加能耗和延迟。
- 攻击场景：
  1. 白盒攻击：已知模型结构和参数，直接优化输入以最小化激活值或最大化运算次数。
  2. 黑盒攻击：
     - 攻击者交互式查询模型，逐步构造恶意输入。Eg：输入中插入无效位（如“A/h/z/9”），迫使模型逐个处理（输入被分割为极小单元），消耗更多资源
     - 攻击者多次重启和迭代优化，将响应时间延迟

**攻击案例：自回归模型**

- 原理：通过序列先前的输入预测下一个元素，形成无限循环
- Eg：GPT2 使用字符对编码进行分词，导致输入长度直接影响计算量
  - 良性输入：输入长度较短，计算高效
  - 恶意输入：输入被分割为极小单元（如单字符+特殊符号），导致token数量激增，计算量指数级增长

**实际影响**

- 自然环境：一次恶意推理的能耗可能比正常推理高30倍。若大规模部署，此类攻击可能导致数据中心能耗激增，加剧碳排放问题。
- 硬件优化失效：现代硬件（GPU）通过并行计算加速模型推理，但恶意输入破坏并行，迫使硬件退化为串行处理

**防御挑战**

- 未知输入检测。传统的输入过滤可能被潜在的恶意输入绕过
- 实时系统需在有限时间内完成推理，无法容忍恶意延迟
- 动态攻击模式。攻击者不断调整攻击策略，防御机制需要持续学习

**防御方向**

- 输入规范化：限制输入长度或token数量 牺牲了模型灵活性
- 资源监控与限流：对异常的请求限流或拒绝服务，但需平衡可用性
- 模型架构优化：设计抗延迟攻击的模型（如固定长度输入）

**简答题**

1. 简述AI可用性攻击的核心目标及典型攻击类型
   - 核心目标：破坏AI系统的及时性和可靠性，导致服务延迟、资源过载或性能降级。
   - 典型攻击类型：
     - 能量延迟攻击：构造恶意输入增加计算负载，延长响应时间
     - 模型投毒：污染训练过程，降低模型整体性能
     - 数据投毒：操纵训练数据，影响模型推理效率

2. 解释“海绵样本”的攻击原理和效果
   - 原理：
     - 构造特殊输入样本，最大化模型计算复杂度（如增加算术操作数、内存访问次数）
     - 利用模型的计算维度特性（如自回归循环、变长输入编码）。
   - 效果：
     - 显著增加推理延迟、提高能耗、造成硬件过热或资源消耗

3. 为什么NLP模型易受能量延迟攻击？结合“计算维度”分析
   - 自回归循环：逐词生成输出导致计算量随输出长度线性增长，攻击者可构造长序列输入。
   - 变长输入编码：恶意输入可被分割为大量短令牌，大幅增加编码和解码的计算负载。
   - 算法复杂度：模型推理设计多层循环操作，攻击样本可触发最坏计算路径

4. 海绵样本的生成方法有哪些？简述遗传算法流程
   - 交互式进化：基于能量/延迟反馈迭代优化样本池。
   - 白盒优化：最小化激活值范数 $\min - ∑ |a_l|_2$。
   - 遗传算法流程（Algorithm 2）：
     a. 初始化随机输入池 $S = \{S_0, S_1, ..., S_n\}$；
     b. 评估这些样本适应度（延迟/能耗）；
     c. 选择最优样本 $S*$；
     d. 按任务类型变异：
        - NLP：拼接样本 $LeftTail(A) + RightTail(B)$ 并随机突变；
        - CV：掩码混合样本 $A × mask + (1 - mask) × B$。

5. 根据NIST标准，对抗性机器学习攻击的分类框架包含哪些维度？
   - 攻击目标：完整性、机密性、可用性。
   - 攻击阶段：训练时（投毒）、推理时（海绵样本）。
   - 攻击者能力：白盒（知悉模型）、黑盒（仅API访问）。
   - 缓解措施：需结合算法鲁棒性、硬件监控及输入过滤。

---



## 📚可解释性

**为什么模型需要可解释性**

- 高风险场景：医疗、金融、司法判决等重要的领域使用到模型必须要求模型给出的结果具有可解释性
- 黑箱问题：深度模型（如DNN，Transformer）的复杂性导致决策过程难以追溯

**可解释性的目标**
- 公平性：预测结果无偏见
- 隐私性：保护隐私数据
- 安全与鲁棒性：防止输入微小变化导致输出大幅波动（对抗样本攻击）
- 因果性：验证模型是否捕捉到因果关系而非相关性
- 信任：用户更愿意信任能解释自身决策的系统

> [!TIP]
>
> 不同利益相关方的角度：
>
> - 用户：了解决策的原因
> - 决策者：验证模型的可靠性
> - 监管机构：评估模型的合规性
> - 研究人员：调试模型，检测偏差

**可解释性的定义**
- Miller的定义：人们能够理解决策原因的程度
- Kim的定义：人们能够一致预测模型结果的程度
- 通用定义：==模型向人类提供可理解的解释的能力==。核心：解释需符合人类认知习惯

**可解释性方法分类**
- 全局解释：理解模型整体行为（某个医疗模型依赖哪些特征做判断）
- 局部解释：解释单个预测结果（为什么某个患者诊断结果是糖尿病）
- 内部行为分析：分析模型内部机制（如注意力权重）
- 事后方法：对已有模型进行解释
- 固有方法：设计时即具备可解释性
- 方法论：可视化、控制单一变量、对比差异、逆向思维

**可解释性方法详解**
1. 透明模型
   - 线性模型：$y = w_1x_1 + w_2x_2 + \cdots + w_nx_n + b$
     - 适合低维数据，直接展示了输入输出之间的关系。每个特征权重 $w_i$ 反应其对预测结果的影响程度。
     - 但是在处理高维问题时难以解释

2. 事后解释方法

   ==LIME：通过局部线性模型近似复杂模型，以线性模型来模拟解释复杂模型==

   - 扰动输入：生成与原输入相似的样本
   - 加权拟合：对扰动样本加权，临近样本权重越高（目的是确保模型更关注原始输入附近区域，忽略原理样本的干扰）
   - 训练简单模型，拟合扰动样本的预测结果。通过局部模型的参数直观解释复杂模型的预测逻辑
   - 公式：$\min_g \mathcal{L}(f, g, \pi_x) + \Omega(g)$
       - $f$：原模型 $g$：局部解释模型
       - $\pi_x$：输入相似权重 $\Omega(g)$：模型复杂度惩罚（如L1正则化）

   ==SHAP：计算每个特征对预测结果的贡献。扰动输入评估全局特征重要性==

   - 公式：$\phi_i = \sum_{S \subseteq F \setminus \{i\}} \frac{|S|!(|F|-|S|-1)!}{|F|!}[f(S \cup \{i\}) - f(S)]$
       - $\phi_i$ 是计算得到每个特征的贡献值。$S$ 是特征子集，$F$是所有特征集合。

   ==Grad-CAM：计算梯度和特征图的加权和，生成热力图，直观展示模型决策依据，颜色深浅代表模型对图像区域的关注程度==

   - 计算梯度：最后一层卷积特征图的梯度进行全局平均池化
   - 加权求和：将梯度权重和特征图相乘，生成热力图
   - 归一化：将热力图和原始图像相加
   - 公式：$L_{Grad-CAM}^c = \text{ReLU}\left(\sum_{k=1}^{K} \alpha_k^c A^k\right)$
     - $\alpha_k^c$：第k个特征图的权重，计算方式：
       - $\alpha_k^c = \frac{1}{Z} \sum_{i,j} \frac{\partial y^c}{\partial A_{ij}^k}$
         - $y^c$：类别c的预测概率。
         - $A_{ij}^k$：第k个特征图在位置(i, j)的值。
     - $A^k$：第k个特征图
   - 适合于图形分类等视觉任务，需要直观可视化模型关注区域

   ==LEMNA：通过非线性近似和特征依赖优化，为复杂的线性模型提供局部可解释性==

   ==提取决策树：读取决策树的分裂规则，得到整体决策依据。在扰动“很小”的情况下，评估某个像素对模型输出的影响==

   - 对于每个像素，分别把它加/减一点点，每次都做一次前向传播（forward pass）计算输出，再看输出变化。
   - 只需做一次反向传播（backward pass），就能得到所有像素对输出分数的“微小贡献”（即梯度），利用反向传播的高效性
   - 缺点：如果图片有成千上万个像素，计算量巨大

   ==积分梯度：从某个基线输入（如均值图像）逐步平滑变成真实输入，在这条路径上累计梯度==

   - 这样能反应出重要特征的真实贡献。
   - 避免传统梯度解释在某些区域不敏感的问题

   其他：
   - Prototype-based：影响函数，目的是找到最能影响某个预测结果的训练样本
   - 缺失特征：通过“有意义的扰动”把关键点抹掉，观察哪个特征是关键点

**可解释性的挑战**
- 对抗样本攻击：通过微小扰动使模型预测出错，导致解释器无法正确解释模型决策
- 解释结果对对抗攻击敏感：可以通过几何修正增强驾驶鲁棒性
- 在一些情况下：准确性和可解释性权衡可能存在反比关系！
  - 因为一些复杂的模型他本身就难以直观解释让人理解，为了提高解释性可能需要简化模型，会导致牺牲准确性




**简答题**
1. 可解释性在AI安全中的核心价值是什么？
   - 高风险决策：帮助决策者理解模型依据，避免偏见
   - 模型调试和偏见检测：识别数据或模型中的潜在偏差
   - 对抗攻击防御：通过解释发现模型脆弱性

2. 局部解释和全局解释的区别？各举一典型方法
   - 局部解释：针对单个样本预测的原因（如“为什么该患者被诊断为糖尿病”），关注局部特征影响。
     - 典型方法：LIME，局部线性近似解释单样本
   - 全局解释：描述模型整体行为逻辑（如“医疗模型主要依赖哪些特征”），揭示宏观规律。
     - 典型方法：SHAP，基于Shapley值量化全局特征的重要性

3. LIME方法的基本原理是什么？它在图像解释中的关键技术挑战是什么？
   - 原理：在目标样本附近扰动生成新数据，训练简单可解释模型（如线性模型）拟合黑盒模型的局部预测，公式：$\min_g \mathcal{L}(f, g, \pi_x) + \Omega(g)$
     - 其中：$g$ 是原模型，$f$ 是要训练的简单模型。
   - 图像挑战：需将像素级输入转化为超像素（语义相似区域）作为解释单元，避免高维像素扰动导致的解释不稳定性。

4. 反事实解释如何提升AI决策的公平性？并给出其数学优化目标。
   - 提供“如果输入特征发生某种变化，模型决策将如何改变”的具体示例，帮助用户理解如何调整自身条件以获得更有利的决策结果。
   - 公平性意义：
     - 避免歧视：揭示决策依赖的非敏感特征（如收入而非种族），减少偏见
     - 赋予追索权：用户可通过可操作的反馈调整行为，而非被动接受结果
   - 数据优化目标：$c = \arg \min_c \left[ y \log(f(c), y) \text{预测一致性} + |x - c| \text{修改距离} \right]$

5. 模型公平性中的“统计平等”与“机会平等”有何区别？
   - 统计平等：要求预测结果独立于敏感属性，如黑人和白人的整体获得贷款率都相同
     - 缺点：可能降低模型性能，如拒绝合格少数群体
   - 机会平等：要求不同群体中的真阳性率相等，如黑人中信用为x的人获得贷款率相同
     - 有点：允许使用群体间差异，更注重实际能力匹配

6. Grad-CAM如何利用梯度信息生成视觉解释？相比CAM有何改进？

   - 原理
     a. 计算目标类别得分 $y^c$ 对末层卷积特征图 $A^k$ 的梯度 $\frac{\partial y^c}{\partial A^k}$
     b. 对梯度空间平均得权重 $\alpha_k^c = \frac{1}{Z} \sum_i \sum_j \frac{\partial y^c}{\partial A_{ij}^k}$。
     c. 加权特征图生成热力图：$L_{Grad-CAM}^c = \text{ReLU} \left( \sum_k \alpha_k^c A^k \right)$
   - 改进：CAM要求网络必须含GAP层，而Grad-CAM利用梯度摆脱此限制，适用于任意CNN架构（如ResNet）。

7. 可解释性方法面临哪些主要挑战？

   - 解释可信度：易受对抗攻击操控

   - 缺乏量化解释质量的评估标准

   - 计算代价大

   - 人类理解差异，不同背景用户对解释的接收程度不同

4. 为何Cynthia Rudin提出“停止可解释ML”？
   - 事后解释可能掩盖模型缺陷
   - 解释和准确性存在权衡（反比关系），应该直接设计为透明模型

---



## 📱应用层安全

**移动端 AI 模型的定义：**
- 在移动设备上运行的 AI 模型

**移动端 AI 模型的特征：**
- 轻量化：模型体积不能过大
- 本地化：无需依赖云服务器，适用于隐私敏感数据
- 实时性：响应时间控制在毫秒级
- 模型结构较简单：以 CNN 居多，运行开销小

**移动端 AI 模型技术难点：**

1. 需要过大的连续内存空间。
   - 解决方案：
     - 分块加载：按需加载模型部分参数
     - 内存优化：利用设备内存管理策略（如内存池分配）
2. 实时性要求高。但是 token-by-token（下一输入依赖于上一个已生成的令牌） 生成方式延迟高。
   - 解决方案：
     - 预加载后续Token，减少等待时间。
     - 硬件加速：利用 NPU/GPU 并行计算
3. 不同厂商芯片差异化，对 NNAPI 的支持不一致
   - 解决方案：
     - 适配层开发：为不同芯片便携兼容代码
     - 适配调度：根据设备自动选择CPU/GPU/NPU
4. 多任务系统干扰：后台进程抢占资源，导致推理中断。
   - 解决方案：
     - 优先级管理：设置 AI 任务为高优先级
     - 容错机制：中断后自动恢复计算
5. 权限限制与 SDK。安卓需要通过 JNI 调用底层接口，但是权限受限
   - 解决方案：
     - 申请特殊权限，如 GPU 调度接口
     - 闭园框架，使用厂商提供的 SDK

**移动端 AI 模型的体积压缩：**
1. 权重量化：将32位浮点数转为8/4位整数，以减少内存占用，降低模型文件大小。局限是一些操作会退回到CPU计算。
2. 权重稀疏化：移除冗余权重，但是模型的精度稍微损失
3. 通道剪枝：在CNN网络中，对特征图中的通道维度进行剪枝，可以降低模型大小和计算复杂度
4. 网络蒸馏：将未压缩的原始模型的输出作为额外的监督信息，指导压缩后的模型的训练
5. 仅训练低秩矩阵，而非全量参数

**移动端 AI 模型的安全设计：**
- 设备端优先：模型仅在设备端运行
- 专用云计算：当需要更高算力时，发送最小必要数据至云端
- 加密传输：HTTPS
- 零存留：云端处理数据立即删除，无法再访问原始内容

**常见的移动端 AI 模型框架：(DL框架)**
- TensorFlow Lite
- NCNN（腾讯研发）
- OneFlow：异构分布式扩展，流式并行，解决集群层面的内存墙挑战
- MindSpore Lite（华为）
- MNN（阿里）

**移动端 AI 模型的安全风险：**
- 数据安全：
  - 生物信息被非法采集：混淆与加密
  - 成员推理攻击
  - 训练数据被污染导致降低了深度学习系统的预测准确率：数据投毒
- 模型参数安全
  - 市面上的很多模型都未采用加密保护措施
- 防御：
  - 对调用代码进行混淆
  - 对模型进行完整性校验
  - 闭源框架
  - 添加水印：防止盗版行为，但无法兼顾模型性能和水印的鲁棒性
  - 加密 AES
  - 将模型的代码功能迁移至Native层实现（C++），因为Native代码比JS代码更难逆向
  - 动态下载，模型不随着应用一起分发，而是在需要时动态下载
  - 自定义OP（算子），使用非标准的、自定义的操作符
  - 将模型转为二进制或bin（bin不能直接打开，但可以使用十六进制编辑器查看）

**张量安全**

- 张量（Tensor）安全：张量是深度学习模型的基本数据结构，操作不当可能导致：
  - 内存泄漏：大量张量操作未释放导致内存耗尽
  - 参数泄露：调用调试接口或日志输出查看张量内容
  - 精度问题：张量量化不当导致模型性能下降
  - 侧信道攻击：观察张量的计算事件时间模型结构
  - 模型窃取：逆向工程提取模型参数 限制模型访问敏感数据
- 防御：
  - 在模型中添加水印：保证模型的来源可追溯，防止被模型被伪造（盗版）
  - 权限控制：限制模型访问敏感数据

**总之：**移动端 AI 的核心矛盾在于性能、安全性与部署效率的平衡。我们需要结合具体场景选择合适的框架，并通过量化、剪枝等技术优化模型体积，同时遵循隐私保护原则。



**简答题**

1. 简述移动端智能模型的主要优势
   - 直接在设备端处理数据，不需要发送网络请求等响应，低延迟且隐私保护
   - 离线可用，适用于偏远地区
   - 节省服务器负载，减少对云端计算资源的依赖
   - 可以本地个性化服务
2. 简述移动端模型使用场景
   - 智能驾驶、人脸识别、指纹识别、扫地机器人导航避障、支付宝扫五福本地化处理
3. 移动端模型部署面临哪些风险？
   - 有被逆向工程或内存dump等方式逆向获取模型参数，恢复模型结构，从而窃取模型的风险
   - 对抗样本攻击通过精心设计输入舞蹈自动驾驶系统
   - 推理过程导致数据隐私如生物特征泄露，被非法采用
4. 移动端模型常见的优化技术有哪些？
   - 将32位浮点数转为8位证书，减少模型大小和计算开销
   - 移除卷积神经网络中冗余通道，降低计算复杂度（通道剪枝）
   - 用大模型知道小模型，以提升小模型性能，也称网络蒸馏
   - 使用移动端的专用框架，NCNN、MNN，加速推理，支持硬件驾驶，GPU

---

## 🧵框架层安全

**框架层安全定义**

- 机器学习框架代码中存在问题和缺陷，致使学习框架在训练或者推理时出现不满足预期的行为
- 可能导致的危害
  - 信息泄露、拒绝服务、恶意代码执行等

**框架缺陷类型**

- 内存非法访问越界
- 空指针引用
- 整数溢出
- 精度丢失，使AI服务无法满足预期要求

**终端机器学习推理框架包括以下两部分**：

- 框架通过模型文件中保存的网络拓扑信息，构建计算图
- 框架通过用户输入数据，使用计算图进行推理，计算最终结果
- *框架中封装了大量计算操作的算法实现：如卷积操作、池化操作、系数操作*



**张量**

- 本质是一个多维数组，是对向量、标量和矩阵的泛化。用于表示数据和模型参数。
- 输入张量突变和随机生成更多的张量来覆盖更多的测试用例，以提升模型的鲁棒性
- 输入张量调度则指的是按一定策略安排输入张量的使用，模拟不同的场景，评估模型在不同情况下的表现和稳定性



**差分测试**

- 指的是比较不同版本之间的差异来检测错误

基于差分测试的深度学习框架测试：

- 挑战：不同框架对API参数有不同的要求和限制，甚至涉及隐式依赖关系。提取这些信息对生成有效测试用例至关重要。生成测试用例后接下来就是测试👇
- 测试：使用转换器和静态分析逆向分析，从而提取不同框架中相似的API，在测试中对其相互对比，得到最异常的框架API
- 测试优化
  - 错误用例根据报错信息不断修复成正确的用例
  - 除测试正常的用例外，还要关注边界情况，范围扩展，测试边界情况

==关注测试用例的代码覆盖率==

**终端设备安全**

- 底层软件安全：AI系统依赖底层软件框架和各种类运行库的支持；与Numpy、openCV等第三方库交互，这些库中隐藏的安全漏洞将影响AI系统的整体安全
- 底层硬件安全：
  - 高通芯片漏洞：允许黑客通过注入恶意代码远程攻击安卓用户
  - 联发科芯片漏洞：导致用户被大规模窃听

**系统衍生安全**

- 数据劫持：在数据传输过程中拦截或篡改数据。比如在人脸识别过程中将摄像头数据替换为视频文件
- 百度人脸安全SDK的防护措施：
  - 对活体图像加密回传
  - 升级网络通信链路协议

安全工具：MindArmour

- 对抗样本检测
- 隐私评估模块



**简答题**

1. 移动端学习框架存在哪些安全风险？
   - 在解析阶段：网络参数错误，导致整数溢出；网络结构错误，相邻层维度不匹配；
   - 在推理阶段：算子实现漏洞，卷积层越界读写
   - 整体包括内存非法访问（堆溢出）、整数溢出、浮点数异常、精度丢失
2. 简述框架缺陷测试方法
   - 差分模糊测试：对比不同框架API的输出差异，暴露不一致性漏洞
   - 张量突变测试用例：覆盖尽可能多的测试路径，增加模型的鲁棒性
3. 终端设备面临哪些软硬件威胁？
   - 软件：第三库漏洞，导致模型崩溃或数据泄露；
   - 硬件：远程代码执行，窃取短信、通话；
   - 设备控制权丢失、隐私泄露、系统拒绝服务
4. 简述三种“数据劫持攻击”的防御错误
   - 数据加密回传，在云端解密，防止中间人穿改
   - 数字签名校验，为明文添加隐藏的签名，后端验证签名一致性，识别伪造数据
   - 网络通信加固，使用安全的DNS解析，双向通讯校验
   - 异常行为监控，检测设备操作环境，拦截非本人操作

---



## ⛓️供应链安全



**AI 供应链安全的定义：**
- 在人工智能系统的整个周期中，从数据收集、模型训练、部署到使用过程中，保护各个环节不被恶意攻击者利用或篡改的安全措施。

**危害：**
- 窃取个人数据
- 在用户电脑上执行恶意代码
- 给出错误的识别结果
- 模型表面上功能正常，实际上在后台执行攻击

**AI 供应链攻击的具体形式**
1. 恶意模型攻击：
   - 定义：攻击者创建或修改模型，使其包含恶意代码
   - 具体例子：
     - 下载了一个图片分类模型
     - 上传一个图片给模型
     - 模型在加载时执行 id 命令（获取系统信息）
     - 甚至删除用户电脑的文件或窃取密码

2. 框架层攻击：
   - 攻击者发现某个 AI 框架（如 TensorFlow 或 PyTorch）某个 API 存在漏洞
   - 攻击者利用这个漏洞执行包含恶意包含恶意代码的模型文件
   - 当用户使用TensorFlow加载这个模型时恶意代码被执行
   
3. 供应链投毒：
   - 在模型训练数据中注入恶意样本
   - 在模型分发过程中替换正常模型
   - 通过开源模型库传播恶意模型

**防御 AI 供应链安全**
1. 静态检测方法：
   - 在不运行模型的情况下，分析模型是否包含可疑内容
   - 具体做法：
     ```python
     # 伪代码示例
     def check_model_safety(model_file):
         # 检查模型文件中是否包含可疑的系统调用
         suspicious_commands = ['os.system', 'subprocess', 'exec', 'eval']
     
         with open(model_file, 'rb') as f:
             content = f.read()
             for cmd in suspicious_commands:
                 if cmd.encode() in content:
                     print(f"警告：发现可疑命令 {cmd}")
                     return False
     ```

2. 动态检测方法：
   - 在安全环境下运行模型，监控其行为，如果检测到系统调用，立即报警
   
     ```python
     # 伪代码示例
     import subprocess
     import threading
     
     def monitor_system_calls():
         # 监控系统调用
         # 如果检测到异常的命令执行，立即报警
         pass
     
     def load_model_safely(model_path):
         # 启动监控线程
         monitor_thread = threading.Thread(target=monitor_system_calls)
         monitor_thread.start()
     
         try:
             # 加载模型
             model = load_model(model_path)
             return model
         except Exception as e:
             print(f"模型加载异常：{e}")
             return None
     ```

3. 对于普通用户
   - 只下载可信来源的 AI 模型
   - 使用知名、经过验证的模型
   - 定期更新 AI 框架和工具

4. 对于开发者
   - 对模型签名验证
   - 建立模型安全检测流程
   - 使用沙箱环境测试新模型（防止全局崩溃）
   - 监控模型运行时的恶意行为



**简答题**

1. 为什么 Pickle 格式不安全
   - 因为 Pickle 反序列化时可执行任意代码，如窃取令牌、篡改仓库、植入后门等，缺乏数字签名和完整性校验
2. 简述 TensorFlow SavedModel 在 Graph 模式下的潜在攻击模式
   - 通过`tf.io.read_file`读取系统敏感文件，导致文件泄露
   - 利用`tf.io.write_file`写入并在终端执行恶意脚本
3. 什么是模型隐写术？如何检测这一类攻击
   - 定义：通过修改模型张量浮点数的低比特位嵌入恶意数据，同时不影响模型表现。
   - 检测方法：
     - 对比原始模型与待测模型的二进制差异
     - 在模型加载运行时动态监控模型的异常行为
4. 列举两种防御供应链攻击措施
   - 加密签名：验证模型来源和完整性
   - 沙箱隔离：在受限的环境中运行不可信的模型

---



## 📧硬件层安全

**AI 硬件层安全的定义**
- 保护运行 AI 模型的硬件设备（主要是 GPU 显卡），防止敏感信息泄露

**LeftoverLocals 漏洞**
- 描述：GPU 内存泄露漏洞：当一个 AI 程序使用完 GPU 后，没有完全清理内存，导致下一个程序可能读到前一个程序的敏感数据
- GPU 内存结构：
  - 全局内存（Global Memory）- 所有程序共享
  - 本地内存（Local Memory）- 单个程序私有
    - 显存：存储 AI 模型参数、输入数据、中间计算结果。但不定时清理，敏感数据可能残留
    - 本地内存：每个 GPU 线程的私有存储空间。但线程结束后可能不会自动清零
    - 纹理内存：优化图像处理的只读内存。但缓存的图像数据可能被其他数据读取
  - 共享内存（Shared Memory）- 同一工作组共享
  - 纹理内存（Texture Memory）- 只读缓存

**云服务攻击**
- 时间线：
  - 10:00 用户 A 在云 GPU 上运行 ChatGPT，处理敏感对话
  - 10:05 用户 A 的任务结束，释放 GPU 资源
  - 10:06 攻击者 B 使用同一个 GPU 资源
  - 10:07 攻击者 B 运行特制程序，读取 GPU 内存残留数据
  - 结果：攻击者可能获取到用户 A 与 ChatGPT 的对话内容

**共享设备攻击**
- 代码块
  - 环境：学校/公司的共享 GPU 服务器
  - 步骤：
    1. 员工 A 使用 GPU 处理机密 AI 任务
    2. 员工 A 任务完成，登出系统
    3. 员工 B（恶意内部人员）使用同一 GPU
    4. 员工 B 运行内存扫描程序
    5. 获取员工 A 处理的机密信息

**硬件层安全的检测方法**
1. OpenCL 内核检测：
   ```python
   def detect_opencl_leftover():
       # 1. 运行一个 OpenCL 程序，在本地内存写入特定数据
       write_test_data_to_local_memory()
   
       # 2. 程序结束
       finish_opencl_kernel()
   
       # 3. 启动新的 OpenCL 程序
       new_kernel = create_new_opencl_kernel()
   
       # 4. 尝试读取本地内存
       leftover_data = read_local_memory()
   
       # 5. 检查是否能读取到之前的数据
       if leftover_data == test_data:
           print("发现 LeftoverLocals 漏洞！")
           return True
       return False
   ```
2. Vulkan 设备内存检测：
   - 代码块
     ```python
     def test_vulkan_memory_isolation():
         # 测试设备本地内存隔离
         device_memory_test()
     
         # 测试共享内存隔离
         shared_memory_test()
     
         # 验证不同程序间是否能互相访问内存
         cross_program_access_test()
     ```

**硬件层安全危害实例**
1. 大语言模型对话泄露
   - 当前用户的会话信息被下一个用户读取到
2. 企业加密泄露
   1. 场景：公司使用 GPU 训练专有 AI 模型
   2. 风险：
      - 模型参数可能残留在 GPU 内存
      - 训练数据可能被其他程序读取
      - 商业机密面临泄露风险

**防护措施**

内存清理：GPU 程序结束时显示清理所有使用过的内存区域



**简答题**

1. 什么事`leftoverLocals`漏洞。其攻击条件是什么？
   - 定义：GPU本地内存未清零导致残留数据泄露，攻击者可窃取其他进程的AI模型数据
   - 条件：多进程共享GPU（如云服务/移动设备），且厂商未实现内存隔离
2. 列举两种防御侧信道攻击的措施
   - 硬件级：启用GPU虚拟化分区，物理隔离，避免多个用户进程共享硬件资源
   - 软件级：强制内核结束时清除本地内存
3. 为什么矩阵乘法操作容易成为LeftoverLocals的攻击目标？
   - 矩阵乘法需缓存大量数据到本地内存，攻击者通过窃取输入向量（如4096×1）结合公开权重（如Hugging Face模型）可重构输出结果

---



## 🤖AI 辅助安全

> [!TIP]
>
> 主题：如何使用机器学习来检测 Windows 恶意软件，攻击者又是如何制造“对抗样本”来欺骗这些检测器的



### 1. 恶意软件与检测基础



**恶意软件定义：**

- 故意设计的程序，目的是造成损害、窃取信息、勒索钱财或控制设备

**传统检测恶意软件的方法：**

- 黑名单：提取文件的签名检查他是否存在已知恶意软件签名库中。缺点是无法检测变种或未知的恶意软件
- 白名单：提取文件的签名检查他是否存在已知安全软件签名库中。缺点是不灵活，可能会导致误检测
- YARA规则：定义一组规则，文件匹配这些规则就被认为是恶意的，能识别恶意软件家族的特征模式。比单纯的哈希值灵活，但是编写规则难度高，且检测性能低



**现存问题：**恶意软件的变种太多太快，传统基于签名和规则的方法已经跟不上，无法检测复杂变种的恶意软件

**机器学习的优势：**

- 从大量数据中学习良性/恶意软件样本，自动区分他们的复杂模式和特征
- 能够识别出它从未见过的恶意软件变种，只要这些变种具有他学习到的 *恶意特征模式*

**两种主要方式：**

1. 静态分析：不运行程序，直接分析程序本身。关注结构、内容、元数据。

   - 特征提取（人工提取）：可解性好

     ```
     导入哪些API函数？（调用了哪些系统功能？）
     
     文件包含哪些段（Sections）？大小如何？（如.text代码段，.data数据段，.rsrc资源段）
     
     文件中的字符串有哪些？
     
     字节直方图/熵（统计字节分布，衡量随机性，加壳/加密文件熵值高）。
     
     文件头信息（PE头信息）。
     ```

   - 特征提取（端到端学习）

     直接把整个文件的原始字节输入给深度学习模型，让模型自己学习哪些字节模式重要。不需要人工设计特征。

   静态分析的优缺点：优点是分析快且不运行文件相对安全。缺点是无法检测运行时才会暴露的恶意行为

2. 动态分析：在隔离的受控环境中运行程序，监控其运行过程中的所有行为

   - 监控内容：

     ```
     调用了哪些API？（创建文件、修改注册表、连接网络等）
     
     访问了哪些文件/注册表项？
     
     连接了哪些IP地址/域名？
     
     创建了哪些进程/线程？
     ```

     将监控报告使用强大的模型来分析序列化行为，判断恶意性

   动态分析的优缺点：能发现运行时行为、对加壳混淆具有抵抗力，但是分析速度慢，有些软件检测到自己被分析的话就不作恶，且沙箱环境无法还原真实环境



### 2. 生成对抗样本

在图像领域：

- 对输入图像添加人眼难以察觉的微小扰动（噪声），就能让训练好的深度神经网络（DNN）完全错误分类

在恶意软件领域：

-  对一个恶意软件文件（`.exe`）进行最小的、保持功能性的修改，使其被ML检测器错误地分类为“良性”

**挑战：**

1. 修改后的文件必须仍然能运行并执行其恶意功能！不能破坏文件结构或代码逻辑；
2. 程序文件是字节序列（离散的），不像图像像素是连续的数值。无法直接做微小的加减（`a` - `b` 没有意义）。距离度量困难。
3. Windows PE文件格式有严格的结构（程序头、节表、节等），修改必须符合规范，否则文件无法加载运行。

**构造对抗样本：**

核心：最小化损失函数，目的是让恶意软件 `x` 被分类器 `f` 识别为良性 `y`
$$
\min_δ L(f(φ(h(x; δ)), y)
$$
其中：

- `h(x; δ)`: 操作函数。在保持功能的前提下，应用参数为`δ`的修改（操作）到文件`x`上。这是恶意软件对抗样本独有的、最核心的部分。
- `φ`: 特征提取函数，分类器使用的，基于特征判断是否为恶意
- `f`：分类器模型
- `y`：目标标签（良性）

具体操作：

1. 研究 PE 文件格式，找到可以安全修改而不影响程序加载和运行的位置

   - 节表偏移修改：修改节表中指向原始数据的指针，在节数据前后或节之间插入内容（*这些插入的内容不被加载到内存*）
   - 节表注入：在节表中添加啊一个新的条目，指向文件末尾添加的内容块，新内容可以被加载到内存或者不加载
   - 填充区利用：每个节的实际数据后面通常用0填充以满足对齐要求。这些0可以被覆盖（加载进内存但不执行）。

2. 选择优化算法：如何找到能成功欺骗分类器的修改参数`δ`？

   白盒模型：

   - 在模型的特征空间计算梯度，指导哪些位置的字节需要改变
   - 在输入空间找到能使嵌入向量沿着梯度方向移动的实际字节替换
   - 迭代修改这些字节，直到成功欺骗模型

   黑盒模型：

   - 进化算法：随机变异（GAMMA：从良性文件中提取良性内容块注入对抗样本中，使其携带强烈的“良性信号”）
   - 迁移攻击：训练一个替代模型，在这个模型上优化生成对抗样本
   - 查询攻击：不断向模型发送对抗样本检测分数，通过分数反馈指导下一步修改



### 3. 防御对抗样本程序

1. 启发式防御/预处理
   - 检查文件是否被已知的对抗操作修改过
   - 使用多个不同的模型进行集成预测
2. 对抗训练
   - 训练阶段：主动加入对抗样本训练。以增加代码的鲁棒性。但可能降低模型在干净样本中的精度
3. 非负神经网络
   - 强制模型所有权重为非负数。直觉是恶意检测应基于“添加”恶意特征（正贡献），而不是“移除”良性特征（负贡献）。希望迫使攻击者只能修改真正重要的内容
4. 单调分类器
   - 设计模型，使得某些关键特征的值只能增加模型的恶意评分（单调递增）。同样基于“添加恶意特征”的直觉。攻击者无法通过修改这些特征来降低分数。
5. 可证明鲁棒的检测器
   - 提供数学证明，保证在输入文件发生特定类型**、**特定数量内的修改（如最多添加K字节）时，模型的预测结果不会改变（即不会被对抗样本欺骗）。这是目前理论上最可靠的防御
6. 对抗样本检测器
   - 不直接替换主检测模型，而是开发一个“插件”或辅助模型，专门用来识别输入文件是否是一个对抗样本



### 4. 局限性

1. 构造对抗样本困难：因为PE格式有严格规范，稍作不慎，程序将无法运行

2. 商业检测模型黑盒：无法获取或测试其内部机制

3. 攻击动态模型更难：（静态攻击修改非代码区域，动态攻击必须修改代码逻辑才能欺骗模型）

   攻击动态分析模型需要修改代码行为（添加垃圾API调用、插入无效分支、代码混淆等），同时保证恶意功能不变。这实际：

   - 地址偏移问题： 注入代码会改变原指令地址，需要处理跳转/调用目标。
   - 可执行段问题： 注入的代码需要放在可执行的内存区域。
   - 重定位问题： 需要考虑操作系统地址空间随机化（ASLR）

| **挑战**               | **静态攻击**                                                 | **动态攻击**                                                 |
| ---------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **地址偏移问题**       | 较少涉及。修改非代码区域（如填充数据）通常不影响代码段地址。 | 必须处理所有跳转/调用指令的地址偏移（如插入代码后，原指令地址可能改变）。 |
| **可执行段问题**       | 注入内容可放在非可执行节（如`.data`），无需执行。            | 注入的“伪装代码”必须位于可执行节（如`.text`），否则会崩溃。  |
| **重定位问题（ASLR）** | 无影响。静态分析不涉及内存地址。                             | 需兼容ASLR：注入代码中的绝对地址需动态计算（如通过`GetProcAddress`获取API地址）。 |
| **行为一致性**         | 无需考虑。只要文件能加载，静态检测器无法验证代码是否实际运行。 | 新增行为必须与原始恶意行为共存（如避免死循环、资源冲突或崩溃）。 |



### 5. 简答题

1. 简述传统恶意检测软件检测方法的缺陷，现代机器学习方法的优势是什么？
   - 传统方法：缺陷
     - 依赖已知签名。而大部分恶意软件只出现一次，无法检测新的变种。
     - 检测规则质量差，且微小的改动即可绕过。
   - 机器学习：优势
     - 从数据中学习攻击模式，可识别未知变种
     - 高效检测：如MalConv直接处理原始字节，GBDT利用人工特征
2. 针对Windows PE文件的对抗攻击需满足哪些条件？列举两种结构操作原理
   - 必要条件：
     - 保持功能不变；避免破坏加载、执行逻辑
     - 利用格式冗余；如未加载区域、对其填充
   - 结构操作：
     1. 区段注入：修改区段表新增条目，该条目指向注入的良性内容，使“良性”元素增多，改内容可选择 是否加载
     2. DOS头扩展：增大`e_lfanew`字段值，在DOS和PE头之间插入任意数据，内存加载但不执行，可以绕过基于头部检测规则
3. 简述现有对抗样本防御方法面临哪些挑战？
   - 对抗训练：生成有效对抗样本困难；需要不断的调整模型来抵抗新的攻击；工业模型闭源导致无法评估其对对抗样本的防御能力
   - 认证鲁棒性：当前只能证明对填充、内容编辑区域攻击的鲁棒性，无法覆盖区段注入等复杂操作
   - 对抗防御代码闭源带来复现困难的问题，导致研究缓慢停滞
   - 工业脱节：企业认为即使对抗样本存在漏洞也不影响产品的整体性和安全性。更关注误报率而非对抗样本带来的风险
